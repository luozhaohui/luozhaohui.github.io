<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>人工智能 on 飘飘白云：所读，所观，所思</title>
    <link>https://luozhaohui.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link>
    <description>Recent content in 人工智能 on 飘飘白云：所读，所观，所思</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 22 Feb 2024 22:14:00 +0800</lastBuildDate><atom:link href="https://luozhaohui.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>人工智能：Sora 随感</title>
      <link>https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/</link>
      <pubDate>Thu, 22 Feb 2024 22:14:00 +0800</pubDate>
      
      <guid>https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/</guid>
      <description>&lt;h2 id=&#34;20250210-更新&#34;&gt;2025.02.10 更新&lt;/h2&gt;
&lt;p&gt;一年前在粗略了解 Sora 之后发表的看法，在对智能与涌现以及 Scaling Law 的理解上有错误和不完善之处，在此更新最新的理解。&lt;/p&gt;
&lt;h3 id=&#34;scaling-law&#34;&gt;Scaling Law&lt;/h3&gt;
&lt;p&gt;对人工智能来说，随着科技的进步，Scaling Law 不是恒定的制约因素，或者说曾经认为的大规模，对现在和将来来说仅仅是小菜一碟。本质上，Scaling Law 和智能的架构并没有多大关系，可以说它仅仅是 AI 发展过程中的一个原料制约因素。当然巧妇难为无米之炊，再精密的饭锅也需要有米才能煮出可口的饭来。&lt;/p&gt;
&lt;h3 id=&#34;智能与涌现&#34;&gt;智能与涌现&lt;/h3&gt;
&lt;p&gt;智能只与 Transformer 相关，而且涌现是必定可能的。涌现是指&lt;strong&gt;在复杂系统中，整体系统展现出无法从个体特性直接预测的新性质&lt;/strong&gt;，这些性质源于个体之间的相互作用，而不是单独个体的功能。Transformer 的如下机制，可以导致涌现或创新想法的出现。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自注意力机制&lt;/strong&gt;：自注意力机制允许模型在多个层次上建立词与词之间的关系。这种多层次关联可能导致模型捕捉到一些隐含的模式或语义结构，从而产生新的理解或生成新的内容。比如：发现藏头诗。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前馈神经网络的非线性变换&lt;/strong&gt;：前馈神经网络通过非线性激活函数对自注意力机制的结果进行非线性变换（使得结果变得更加发散）。这种非线性变换可能导致模型捕捉到一些复杂的语言模式，从而生成新的内容。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多层堆叠的深度结构&lt;/strong&gt;：Transformer 通常由多个层堆叠而成，每一层都会对输入进行进一步的处理。这种多层结构可能导致模型捕捉到更高层次的语义特征，从而产生新的理解或生成新的内容。也就是说多层累积降低了有限方案的确定性，增加了发散性。比如说每一层唯一方案的确定性为 95%，那么经六层累积之后，这个确定性就降为 74% 了（95% 的 6 次方 = 73.51%），也就是说有 26% 的联想发挥和知识迁移空间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自回归生成机制&lt;/strong&gt;：在生成任务中，Transformer 通过自回归的方式逐步生成文本。每一步生成都依赖于之前生成的内容，这种动态过程可能导致模型“变异”走上不寻常路，并逐步“自反馈”式地 在这条变异路径上演变，产生一些意想不到的结果。这有点像演化论中的变异累积过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通俗地说，Transformer 设计中的智能或者涌现体现在“要有想法又要想法不荒谬”或者“要浪又不要太浪”；用数学术语说就是“要发散又要能收敛”，发散才能打开联想空间，收敛才能聚焦目的，不至于过于天马行空；用 Transformer 术语来说就是“要梯度不能消失又要不能爆炸”。可以用头脑风暴来进行类比：第一阶段要发散思维，无需考虑任何约束条件。这相当于上面提到的众多机制。第二阶段要对第一阶段的结果进行评估和收敛。这相当于层归一化和优势函数所起的作用。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
