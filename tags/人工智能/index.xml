<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>人工智能 on 飘飘白云：所读，所观，所思</title>
    <link>https://luozhaohui.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link>
    <description>Recent content in 人工智能 on 飘飘白云：所读，所观，所思</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 15 Mar 2025 22:14:00 +0800</lastBuildDate><atom:link href="https://luozhaohui.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>人工智能发展之潮起潮落</title>
      <link>https://luozhaohui.github.io/post/2025/2025-03-15-history-of-ai/</link>
      <pubDate>Sat, 15 Mar 2025 22:14:00 +0800</pubDate>
      
      <guid>https://luozhaohui.github.io/post/2025/2025-03-15-history-of-ai/</guid>
      <description>&lt;h2 id=&#34;引子&#34;&gt;引子&lt;/h2&gt;
&lt;p&gt;借 Deepseek 的东风，打算深入了解一下人工智能，因而读了《&lt;a href=&#34;https://book.douban.com/subject/30379536/&#34;&gt;智慧的疆界：从图灵机到人工智能&lt;/a&gt;》、《&lt;a href=&#34;https://book.douban.com/subject/35418482/&#34;&gt;人工智能全传&lt;/a&gt;》、《&lt;a href=&#34;https://book.douban.com/subject/36668702/&#34;&gt;GPT图解&lt;/a&gt;》等一批人工智能相关的书籍。《智慧的疆界》是技术派作家周志明用心著作的一本人工智能科普书籍。在此书中作者注入了自己的思考，并将思考所得的人工智能发展脉络、哲学上的思索、道德上的褒贬一并展现出来。因此内容不像某些所谓的人工智能教授搜罗堆砌名词术语的书籍那样空大乏，但在文字叙述的达和雅上还欠些火候。引用一下豆瓣上对此书名副其实的简介：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;从奠基人物、历史事件、学术理论、研究成果、技术应用等5个维度全面读懂人工智能。本书以时间为主线，用专业的知识、通俗的语言、巧妙的内容组织方式，详细讲解了人工智能这个学科的全貌、能解决什么问题、面临怎样的困难、尝试过哪些努力、取得过多少成绩、未来将向何方发展，尽可能消除人工智能的神秘感，把阳春白雪的人工智能从科学的殿堂推向公众面前。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;站在这些前人的肩膀上，以及人工智能最新的进展，在此整理一番人工智能的发展历程，并加入跨学科的思考所得。&lt;/p&gt;
&lt;h2 id=&#34;理论开端&#34;&gt;理论开端&lt;/h2&gt;
&lt;h3 id=&#34;可形式化与可计算&#34;&gt;可形式化与可计算&lt;/h3&gt;
&lt;p&gt;1900年，数学家&lt;strong&gt;大卫·希尔伯特&lt;/strong&gt;于向国际数学界提出了著名的“&lt;strong&gt;二十三个数学问题&lt;/strong&gt;”，其中第二个和第十个问题与人工智能的&lt;strong&gt;理论可能性&lt;/strong&gt;和&lt;strong&gt;实现可能性&lt;/strong&gt;紧密相关：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题二：&lt;strong&gt;算术公理的一致性证明&lt;/strong&gt;：证明在算术系统中，即在基本的算术运算（加法、乘法等）和数的概念基础上，不存在能够同时成立又相互矛盾的定理。&lt;/li&gt;
&lt;li&gt;问题十：&lt;strong&gt;丢番图方程的可解性判定&lt;/strong&gt;：是否存在一种算法，可以通过有限步骤判断任意给定的丢番图方程（Diophantine equation）是否有整数解。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1931年&lt;strong&gt;库尔特·哥德尔&lt;/strong&gt;提出了著名的&lt;strong&gt;哥德尔不完备定理&lt;/strong&gt;证否问题二。哥德尔不完备定理表明，在任何足够复杂的形式系统中，&lt;strong&gt;总存在一些命题无法在这个系统内部被证明或证伪，即该系统是不完备的，一定存在逻辑矛盾的表述&lt;/strong&gt;。典型示例如语言符号系统中的&lt;strong&gt;说谎者悖论&lt;/strong&gt;：“这句话是假的”。&lt;/p&gt;
&lt;p&gt;1970年&lt;strong&gt;尤里·马季亚谢维奇&lt;/strong&gt;证明了无法通过固定的算法步骤来判断所有丢番图方程是否具有整数解，从而证否了问题十。问题十和图灵停机问题是等价的。&lt;strong&gt;图灵停机问题&lt;/strong&gt;（Halting Problem）是图灵于1936年提出的，该问题是说：是否存在一个算法对于任意给定的程序和输入，可以判定程序最终会停止运行（停机）还是会无限期地运行下去。图灵自己通过反证法证明图灵停机问题是不可判定的。&lt;/p&gt;
&lt;p&gt;这两个问题都在理论上被证否了，也就是说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不是所有问题都可以被形式系统所表征。也就是说&lt;strong&gt;形式系统不能抽象所有现实问题&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;不能所有被形式系统表征的问题都可以通过计算来解决。也就是说&lt;strong&gt;计算不能解决所有可形式化的问题&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于问题的范围大于可形式化的范围，而可形式化的范围又大于可计算的范围，所以&lt;strong&gt;可形式化和可计算理论就限定了机器计算的边界&lt;/strong&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>人工智能：对涌现与智能的再思考</title>
      <link>https://luozhaohui.github.io/post/2025/2025-02-15-thinking-about-agi/</link>
      <pubDate>Sat, 15 Feb 2025 22:14:00 +0800</pubDate>
      
      <guid>https://luozhaohui.github.io/post/2025/2025-02-15-thinking-about-agi/</guid>
      <description>&lt;h3 id=&#34;引子&#34;&gt;引子&lt;/h3&gt;
&lt;p&gt;一年前在粗略了解 Sora 之后发表了对通用人工智能（AGI） 的粗浅看法（见&lt;a href=&#34;https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/&#34;&gt;人工智能：Sora 随感&lt;/a&gt;)，经过对 AGI 知识的学习之后，再次刷新对 Scaling Law、Transformer 以及智能/涌现的理解，因此有了此文以更新对这个知识点的“模型参数”（一年前我对 AGI 能否达到涌现是存疑的，现在转变为肯定）。都说大道至简，但精准简化岂是常人所能，我只好在通俗与严谨之间，弃严谨而取通俗。下面就是用通俗的方式来讲述我对人工智能架构主要思想的新理解。&lt;/p&gt;
&lt;h3 id=&#34;scaling-law-与涌现&#34;&gt;Scaling Law 与涌现&lt;/h3&gt;
&lt;p&gt;Scaling Law 描述了数据规模、计算资源和模型参数对 AI 模型的影响。大模型之所要大，是因为只有当模型的训练（样本）数据和参数大到突破一定的临界值后，才可能涌现出一些不可预测、更复杂的能力和特性，而进行这样大规模的训练又依赖于大规模的计算资源。这等规模的模型能够从原始训练数据中自动学习并发现或发明新的、更高层次的特征和模式，这种能力被称为“&lt;strong&gt;涌现（Emergence）&lt;/strong&gt;”。随着科技的进步，曾经被认为难以突破的计算和数据限制，将来一定会是可控和可实现的。从本质上讲，Scaling Law 不是决定 AI 智能形态的根本因素，而是影响其发展的&lt;strong&gt;资源门槛&lt;/strong&gt;。但就像“巧妇难为无米之炊”，即使有再先进的模型架构，也需要规模足够大的数据和计算能力来达到涌现所需的阈值。此外，涌现现象的出现通常依赖于大规模个体之间的相互作用，只有在规模足够大的个体之间的&lt;strong&gt;非线性关系&lt;/strong&gt;才可能催生出&lt;strong&gt;整体大于部分之和&lt;/strong&gt;的智能表现。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>人工智能：Sora 随感</title>
      <link>https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/</link>
      <pubDate>Thu, 22 Feb 2024 22:14:00 +0800</pubDate>
      
      <guid>https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/</guid>
      <description>&lt;h2 id=&#34;sora-做了什么&#34;&gt;Sora 做了什么&lt;/h2&gt;
&lt;p&gt;基于目前公开的信息 Sora 模型奠基于两大原理之上：Transformer 与 Scaling Law。前者本质就是换个角度看问题，使得跨领域建模及其处理更为一致、高效，一如：频率和概率，直角坐标系和极坐标系，大数据常用的矩阵变换等等；后者就是大力出奇迹的规模效应：模型大，数据多，算力强，那效果就更好（是否能达到涌现的程度存疑）。个人感觉前者使“通用”成为可能，后者使“智能”成为可能。Sora 强大之处是仅通过学习人类产生的视频（仅视觉、听觉、字幕），就能产生&lt;strong&gt;世界模拟器&lt;/strong&gt;的四维视频。但我不认为它从中推理出牛顿定律或能量守恒定律等，进而根据这些理论来构建四维世界视频，它仅仅是通过大数据分析在可能性空间中选择相关关系最大的情景。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
