<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[标签：工具 | 飘飘白云]]></title>
  <link href="http://luozhaohui.github.io/tags/gong-ju/atom.xml" rel="self"/>
  <link href="http://luozhaohui.github.io/"/>
  <updated>2017-03-31T17:46:50+08:00</updated>
  <id>http://luozhaohui.github.io/</id>
  <author>
    <name><![CDATA[飘飘白云]]></name>
    <email><![CDATA[kesalin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[利用 Synergy 在 Mac 与 Ubuntu 间共享键盘与鼠标]]></title>
    <link href="http://luozhaohui.github.io/blog/2014/12/08/share-screen-with-synergy/"/>
    <updated>2014-12-08T13:03:34+08:00</updated>
    <id>http://luozhaohui.github.io/blog/2014/12/08/share-screen-with-synergy</id>
    <content type="html"><![CDATA[<h2>简介</h2>

<p>Synergy 是一款相当不错的跨平台共享桌面的工具软件，我时常需要在 Mac &amp; Ubuntu &amp; Windows 等系统之间同步工作，因而利用这个这个工具能够大大提供生产效率。由于 Synergy 没有 UI 界面，程序员们开发了各种带界面的开源版本，如 QuickSynergy 或 SynergyKM 等。QuickSynergy 与 SynergyKM 可以混合搭配使用，即任选一个用作服务器，另一个用作客户端程序也是可以运行成功的。但是要<strong>注意各机器上的 Synergy 版本要一致</strong>，否则会出现不同版本不兼容导致无法链接通信！这个问题折腾来我好一会儿，通过安装 SynergyKM 查看日志才发现问题所在。</p>

<p>在本例中，使用 QuickSynergy 作为客户端和服务器，作为服务器的机器安装 Ubuntu 系统，客户端机器安装 Mac 系统。</p>

<!--more-->


<h2>工具下载</h2>

<p>QuickSynergy 工具下载：<br/>
Mac 版 QuickSynergy 下载链接一：<a href="https://code.google.com/p/quicksynergy/">https://code.google.com/p/quicksynergy/</a>  <br/>
Mac 版 QuickSynergy 下载链接二：<a href="http://sourceforge.net/projects/quicksynergy/">http://sourceforge.net/projects/quicksynergy/</a><br/>
Ubuntu 版 QuickSynergy 直接在 Ubuntu Software Center 中输入 QuickSynergy 查找安装即可。</p>

<p>若使用 SynergyKM，可以从如下链接获取：<br/>
SynergyKM 下载链接：<a href="http://synergykm.com/">http://synergykm.com/</a></p>

<h2>安装配置</h2>

<ol>
<li><p>Ubuntu 下的安装配置</p>

<p> 启动 QuickSynergy，显示界面有三个 Tab 页：</p>

<ul>
<li>Share Tab：该页面用于服务器。作为服务器的机器在该界面配置受其控制的客户端机器的主机名，然后点击 Excute 启动服务器端（实际上是执行程序：<code>/usr/bin/synergys</code>）；</li>
<li>Use Tag：该页面用于客户端。作为客户端的机器在该界面配置服务器机器的IP地址或主机名，然后点击 Excute 启动客户端端（实际上是执行程序：<code>/usr/bin/synergyc</code>）；</li>
<li>Setting Tag：该页面用于配置 Synergy 执行程序所在的目录</li>
</ul>


<p> 在这里，使用 Ubuntu 机器作为服务器，所以只需要在 Share Tab 界面上的相应位置（上下左右，我的 Mac 机器在左边，所以在 left 位置设置）设置 Mac 机器的主机名即可。Mac 机器的主机名查看方法如下：启动 terminal，输入命令：<code>hostname</code>，就能得到主机名。</p>

<p> 设置界面如下：<br/>
 <img src="http://i.imgur.com/ZhCLJVS.png" alt="Ubuntu QuickSynergy 设置" /></p>

<p> 然后点击启动。(注意需要先启动服务器端，然后再启动客户端)</p></li>
<li><p>Mac 下的安装配置
Mac 版 QuickSynergy 界面比 Ubuntu 版要简洁一些，启动 QuickSynergy，在 Share 页面设置 Server hostname/IP address 为 Ubuntu 机器的 IP 地址即可（Ubuntu 系统下启动 terminal，输入命令：<code>ifconfig</code> 即可查看 IP 地址，输入命令 <code>hostname</code> 即可查看主机名）。</p>

<p>  设置界面如下：<br/>
  <img src="http://i.imgur.com/kKxpgnx.png" alt="Mac QuickSynergy 设置" /></p>

<p>  然后点击 Run，这样就能够使用 Ubuntu 机器的键盘和鼠标操作 Mac 机器了。</p></li>
<li><p>Mac 下 SynergyKM 的安装配置<br/>
由于 QuickSynergy 不带日志输出窗口，有时候操作不成功也不知道是怎么回事，这时可以安装 SynergyKM 来解决问题。下载 Mac 版 SynergyKM，双击安装，在安装过程中选择 Client 模式，并设置 Screen Name 为 Mac 机器的主机名（也可以在安装完毕之后在菜单 Perferences/Setting 界面设置）。安装完毕之后启动 SynergyKM，勾选 Client 选择框，在 Server IP 处填写服务器机器 Ubuntu 的 IP 地址。</p>

<p>  设置界面如下：
  <img src="http://i.imgur.com/nNcKVPg.png" alt="Mac SynergyKM 设置" /></p>

<p>  然后点击 Apply 和 Start，就能在 Log 输出窗口查看日志，根据日志信息就能够知道是版本不匹配还是主机名设置错误等等。</p></li>
<li><p>修改 key-Mapping<br/>
由于 Mac 机键盘与非 Mac 机键盘的物理按键不同，所以在 Ubuntu 机器上的 Ctrl + C 并不能在 Mac 系统上产生同样的效果，这就需要在 Synergy 的配置文件中添加按键映射关系。首先关闭 Ubuntu 系统上的 QuickSynergy（每次启动 QuickSynergy 都会重新生成新的 synergy.conf），再在作为服务器的机器上的 terminal 中输入命令：<code>gedit ~/.quicksynergy/synergy.conf</code> 打开 synergy.conf，修改其内容如下：
<code>
section: screens
 kesalin-ubuntu:
 kesalin-iMac:
     ctrl = meta
     meta = ctrl
     alt = alt
     super = super
end
section: links
 kesalin-ubuntu:
     left = kesalin-iMac
 kesalin-iMac:
     right = kesalin-ubuntu
end
</code></p></li>
</ol>


<p>然后在 terminal 上输入命令：<code>/usr/bin/synergys -c ~/.quicksynergy/synergy.conf</code> 根据特定配置启动 synergys 服务端，就可以实现按键映射了。</p>

<p>为了避免每次启动系统之后都需要敲入上面的启动命令，可以在 Ubuntu 的启动项中添加上面的命令：点击 Perference->Startup Applications Perferences->Add，在 command 栏输入：<code>/usr/bin/synergys -c ~/.quicksynergy/synergy.conf</code> 即可。注意由于每次启动 QuickSynergy 都会重新生成新的 synergy.conf 覆盖我们修改好的配置文件，所以最好将修改好的 synergy.conf 拷贝备份为 synergy-keymap.conf，然后使用命令：<code>/usr/bin/synergys -c ~/.quicksynergy/synergy-keymap.conf</code> 作为自启动命令。</p>

<h2>参考链接</h2>

<p><a href="https://help.ubuntu.com/community/SynergyHowto">SynergyHowto</a><br/>
<a href="http://superuser.com/questions/90223/synergy-key-mapping">Synergy key mapping</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Python编写博客导出工具]]></title>
    <link href="http://luozhaohui.github.io/blog/2014/10/12/export-blog-using-python-on-windows/"/>
    <updated>2014-10-12T08:35:02+08:00</updated>
    <id>http://luozhaohui.github.io/blog/2014/10/12/export-blog-using-python-on-windows</id>
    <content type="html"><![CDATA[<h3>写在前面的话</h3>

<p>我在 github 上用 octopress 搭建了<a href="http://kesalin.github.io/">个人博客</a>，octopress 使用Markdown语法编写博文。之前我在CSDN博客上也写过不少的技术博文，都说自己的孩子再丑也是个宝，所以就起了把CSDN博客里面的文章导出到个人博客上的念头。刚开始想找个工具把CSDN博客导出为xml或文本，然后再把xml或文本转换为Markdown博文。可惜搜了一下现有博客导出工具，大部分要收费才能将全部博文导出为xml格式，所以就只好发明轮子了：写个工具将全部博文导出为Markdown博文（也是txt格式的）。</p>

<p>我将详细介绍这个工具的编写过程，希望没有学习过编程的人也能够学会一些简单的Python语法来修改这个脚本工具，以满足他们将其他类型的博客导出为文本格式。这也是我第一次学习和使用Python，所以相信我，你一定也可以将自己的博客导出为想要的文本格式。</p>

<p>本文源代码在这里：<a href="https://github.com/kesalin/PythonSnippet/blob/master/ExportCSDNBlog.py">ExportCSDNBlog.py</a></p>

<!--more-->


<p>考虑到大部分非程序员使用Windows系统，下面将介绍在Windows下如何编写这个工具。</p>

<h3>下载工具</h3>

<p>在 Windows 下安装Python开发环境（Linux/Mac下用pip安装相应包即可，程序员自己解决咯）：</p>

<p><strong>Python 2.7.3</strong><br/>
请安装这个版本，更高版本的Python与一些库不兼容。<br/>
<a href="https://www.python.org/download/releases/2.7.3/">下载页面</a><br/>
下载完毕双击可执行文件进行安装，默认安装在C:\Python2.7。</p>

<p><strong>six</strong><br/>
<a href="https://pypi.python.org/pypi/six">下载页面</a>
下载完毕，解压到Python安装目录下，如C:\Python2.7\six-1.8.0目录下。</p>

<p><strong>BeautifulSoup 4.3.2</strong><br/>
<a href="http://www.crummy.com/software/BeautifulSoup/bs4/download/">下载页面</a>，
下载完毕，解压到Python安装目录下，如C:\Python2.7\BeautifulSoup目录下。</p>

<p><strong>html5lib</strong><br/>
<a href="https://pypi.python.org/pypi/html5lib">下载页面</a>
下载完毕，解压到Python安装目录下，如C:\Python2.7\html5lib-0.999目录下。</p>

<h3>安装工具</h3>

<p>Windows下启动命令行，依次进入如下目录，执行setup.py install进行安装：</p>

<pre><code>C:\Python2.7\six-1.8.0&gt;setup.py install  
C:\Python2.7\html5lib-0.999&gt;setup.py install  
C:\Python2.7\BeautifulSoup&gt;setup.py install  
</code></pre>

<h3>参考文档</h3>

<p><a href="https://docs.python.org/2/">Python 2.X文档</a><br/>
<a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html">BeautifulSoup文档</a><br/>
<a href="http://www.regexlab.com/zh/regref.htm">正则表达式文档</a><br/>
<a href="http://tool.oschina.net/regex">正则表达式在线测试</a></p>

<h3>用到的Python语法</h3>

<p>这个工具只用到了一些基本的Python语法，如果你没有Python基础，稍微了解一下如下博文是很有好处的。</p>

<ul>
<li>string: 字符串操作，参考<a href="http://www.cnblogs.com/sevenyuan/archive/2010/12/10/1902145.html">python: string的操作函数</a></li>
<li>list: 列表操作，参考<a href="http://www.cnblogs.com/zhengyuxin/articles/1938300.html">Python list 操作</a></li>
<li>dictionary: 字典操作，参考<a href="http://www.cnblogs.com/yangyongzhi/archive/2012/09/17/2688326.html">Python中dict详解</a></li>
<li>datetime: 日期时间，参考<a href="http://www.cnblogs.com/lhj588/archive/2012/04/23/2466653.html">python datetime处理时间</a></li>
</ul>


<p><br/></p>

<h3>编写博客导出工具</h3>

<h4>分析</h4>

<p>首先来分析这样一个工具的需求：</p>

<pre><code>导出所有CSDN博客文章为Markdown文本。
</code></pre>

<p>这个总需求其实可以分两步来做：</p>

<pre><code>* 获得CSDN博客文章
* 将文章转换为Markdown文本
</code></pre>

<p>针对第一步：如何获取博客文章呢？</p>

<p>打开任何一个CSDN博客，我们都可以看到下方的页面导航显示“XXX条数据 共XXX页 1 2 3 &hellip; 尾页”，我们从这个地方入手考虑。每个页面上都会显示属于该页的文章标题及文章链接，如果我们依次访问这些页面链接，就能从每个页面链接中找出属于该页面的文章标题及文章链接。这样所有的文章标题以及文章链接就都获取到了，有了这些文章链接，我们就能获取对应文章的html内容，然后通过解析这些html页面来生成相应Markdown文本了。</p>

<h4>实现</h4>

<p>从上面的分析可以看出，首先我们需要根据首页获取所有的页面链接，然后遍历每一个页面链接来获取文章链接。</p>

<ul>
<li>获取页面链接的代码：</li>
</ul>


<pre><code class="python 获取所有的页面的 url https://github.com/kesalin/PythonSnippet/blob/master/ExportCSDNBlog.py View Source">def getPageUrlList(url):
    # 获取所有的页面的 url
    request = urllib2.Request(url, None, header)
    response = urllib2.urlopen(request)
    data = response.read()

    #print data
    soup = BeautifulSoup(data)

    lastArticleHref = None
    pageListDocs = soup.find_all(id="papelist")
    for pageList in pageListDocs:
        hrefDocs = pageList.find_all("a")
        if len(hrefDocs) &gt; 0:
            lastArticleHrefDoc = hrefDocs[len(hrefDocs) - 1]
            lastArticleHref = lastArticleHrefDoc["href"].encode('UTF-8')

    if lastArticleHref == None:
        return []

    #print " &gt; last page href:" + lastArticleHref
    lastPageIndex = lastArticleHref.rfind("/")
    lastPageNum = int(lastArticleHref[lastPageIndex+1:])
    urlInfo = "http://blog.csdn.net" + lastArticleHref[0:lastPageIndex]

    pageUrlList = []
    for x in xrange(1, lastPageNum + 1):
        pageUrl = urlInfo + "/" + str(x)
        pageUrlList.append(pageUrl)
        log(" &gt; page " + str(x) + ": " + pageUrl)

    log("total pages: " + str(len(pageUrlList)) + "\n")
    return pageUrlList
</code></pre>

<p>参数 url = &ldquo;<a href="http://blog.csdn.net/">http://blog.csdn.net/</a>&rdquo; + username，即你首页的网址。通过urllib2库打开这个url发起一个web请求，从response中获取返回的html页面内容保存到data中。你可以被注释的 print data 来查看到底返回了什么内容。</p>

<p>有了html页面内容，接下来就用BeautifulSoup来解析它。BeautifulSoup极大地减少了我们的工作量。我会详细在这里介绍它的使用，后面再次出现类似的解析就会从略了。soup.find_all(id=&ldquo;papelist&rdquo;) 将会查找html页面中所有id=&ldquo;papelist"的tag，然后返回包含这些tag的list。对应 CSDN 博文页面来说，只有一处地方：</p>

<p>``` html papelist html内容示例</p>

<div id="papelist" class="pagelist">
    <span> 236条数据  共12页</span>
    <strong>1</strong> 
    <a href="http://luozhaohui.github.io/kesalin/article/list/2">2</a>
    <a href="http://luozhaohui.github.io/kesalin/article/list/3">3</a>
    <a href="http://luozhaohui.github.io/kesalin/article/list/4">4</a> 
    <a href="http://luozhaohui.github.io/kesalin/article/list/5">5</a> 
    <a href="http://luozhaohui.github.io/kesalin/article/list/6">...</a> 
    <a href="http://luozhaohui.github.io/kesalin/article/list/2">下一页</a> 
    <a href="http://luozhaohui.github.io/kesalin/article/list/12">尾页</a>
</div>


<pre><code class="">
好，我们获得了papelist 的tag对象，通过这个tag对象我们能够找出尾页tag a对象，从这个tag a解析出对应的href属性，获得尾页的编号12，然后自己拼出所有page页面的访问url来，并保存在pageUrlList中返回。page页面的访问url形式示例如下：

    &gt; page 1: http://blog.csdn.net/kesalin/article/list/1

* 根据page来获取文章链接的代码：
</code></pre>

<p>def getArticleList(url):
    # 获取所有的文章的 url/title
    pageUrlList = getPageUrlList(url)</p>

<pre><code>articleListDocs = []

strPage = " &gt; parsing page {0}"
pageNum = 0
global gRetryCount
for pageUrl in pageUrlList:
    retryCount = 0
    pageNum = pageNum + 1
    pageNumStr = strPage.format(pageNum)
    print pageNumStr

    while retryCount &lt;= gRetryCount:
        try:
            retryCount = retryCount + 1
            time.sleep(1.0) #访问太快会不响应
            request = urllib2.Request(pageUrl, None, header)
            response = urllib2.urlopen(request)
            data = response.read().decode('UTF-8')

            #print data
            soup = BeautifulSoup(data)

            topArticleDocs = soup.find_all(id="article_toplist")
            articleDocs = soup.find_all(id="article_list")
            articleListDocs = articleListDocs + topArticleDocs + articleDocs
            break
        except Exception, e:
            print "getArticleList exception:%s, url:%s, retry count:%d" % (e, pageUrl, retryCount)
            pass

artices = []
topTile = "[置顶]"
for articleListDoc in articleListDocs:
    linkDocs = articleListDoc.find_all("span", "link_title")
    for linkDoc in linkDocs:
        #print linkDoc.prettify().encode('UTF-8')
        link = linkDoc.a
        url = link["href"].encode('UTF-8')
        title = link.get_text().encode('UTF-8')
        title = title.replace(topTile, '').strip()
        oneHref = "http://blog.csdn.net" + url
        #log("   &gt; title:" + title + ", url:" + oneHref)
        artices.append([oneHref, title])

log("total articles: " + str(len(artices)) + "\n")
return artices
</code></pre>

<pre><code class="">
从第一步获得所有的page链接保存在pageUrlList中，接下来就根据这些page 页面来获取对应page的article链接和标题。关键代码是下面这三行：
</code></pre>

<pre><code>topArticleDocs = soup.find_all(id="article_toplist")
articleDocs = soup.find_all(id="article_list")
articleListDocs = articleListDocs + topArticleDocs + articleDocs
</code></pre>

<pre><code class="">
从page的html内容中查找置顶的文章（article_toplist）以及普通的文章（article_list）的tag对象，然后将这些tag保存到articleListDocs中。

article_toplist示例：(article_list的格式是类似的)
</code></pre>

<div id="article_toplist" class="list">
    <div class="list_item article_item">
        <div class="article_title">   
            <span class="ico ico_type_Original"></span>
            <h1>
                <span class="link_title">
                <a href="http://luozhaohui.github.io/kesalin/article/details/10474007">
                <font color="red">[置顶]</font>
                招聘：有兴趣做一个与Android对等的操作系统么？
                </a>
                </span>
            </h1>
        </div>
        ... ...
    </div>
    ... ...
</div>


<pre><code class="">
然后遍历所有的保存到articleListDocs里的tag对象，从中解析出link_title的span tag对象保存到linkDocs中；然后从中解析出链接的url和标题，这里去掉了置顶文章标题中的“置顶”两字；最后将url和标题保存到artices列表中返回。artices列表中的每一项内容示例：

       title:招聘：有兴趣做一个与Android对等的操作系统么？  
       url:http://blog.csdn.net/kesalin/article/details/10474007

* 根据文章链接获取文章html内容并解析转换为Markdown文本
</code></pre>

<p>def download(url, output):
    # 下载文章，并保存为 markdown 格式
    log(&ldquo; >> download: &rdquo; + url)</p>

<pre><code>data = None
title = ""
categories = ""
content = ""
postDate = datetime.datetime.now()

global gRetryCount
count = 0
while True:
    if count &gt;= gRetryCount:
        break
    count = count + 1
    try:
        time.sleep(2.0) #访问太快会不响应
        request = urllib2.Request(url, None, header)
        response = urllib2.urlopen(request)
        data = response.read().decode('UTF-8')
        break
    except Exception,e:
        exstr = traceback.format_exc()
        log(" &gt;&gt; failed to download " + url + ", retry: " + str(count) + ", error:" + exstr)
        pass

if data == None:
    log(" &gt;&gt; failed to download " + url)
    return

#print data
soup = BeautifulSoup(data)

topTile = "[置顶]"
titleDocs = soup.find_all("div", "article_title")
for titleDoc in titleDocs:
    titleStr = titleDoc.a.get_text().encode('UTF-8')
    title = titleStr.replace(topTile, '').strip()
    #log(" &gt;&gt; title: " + title)

manageDocs = soup.find_all("div", "article_manage")
for managerDoc in manageDocs:
    categoryDoc = managerDoc.find_all("span", "link_categories")
    if len(categoryDoc) &gt; 0:
        categories = categoryDoc[0].a.get_text().encode('UTF-8').strip()

    postDateDoc = managerDoc.find_all("span", "link_postdate")
    if len(postDateDoc) &gt; 0:
        postDateStr = postDateDoc[0].string.encode('UTF-8').strip()
        postDate = datetime.datetime.strptime(postDateStr, '%Y-%m-%d %H:%M')

contentDocs = soup.find_all(id="article_content")
for contentDoc in contentDocs:
    htmlContent = contentDoc.prettify().encode('UTF-8')
    content = htmlContent2String(htmlContent)

exportToMarkdown(output, postDate, categories, title, content)
</code></pre>

<pre><code class="">
同前面的分析类似，在这里通过访问具体文章页面获得html内容，从中解析出文章标题，分类，发表时间，文章内容信息。然后把这些内容传递给函数exportToMarkdown，在其中生成相应的Markdown文本文件。值得一提的是，在解析文章内容信息时，由于html文档内容有一些特殊的标签或转义符号，需要作特殊处理，这些特殊处理在函数htmlContent2String中进行。目前只导出了所有的文本内容，图片，url链接以及表格都没有处理，后续我会尽量完善这些转换。
</code></pre>

<p>def htmlContent2String(contentStr):
    patternImg = re.compile(r'(&lt;img.+?src=&ldquo;)(.+?)(&rdquo;.+ />)&lsquo;)
    patternHref = re.compile(r&rsquo;(&lt;a.+?href=&ldquo;)(.+?)(&rdquo;.+?>)(.+?)(</a>)&lsquo;)
    patternRemoveHtml = re.compile(r&rsquo;&lt;/?[^>]+>&lsquo;)</p>

<pre><code>resultContent = patternImg.sub(r'![image_mark](\2)', contentStr)
resultContent = patternHref.sub(r'[\4](\2)', resultContent)
resultContent = re.sub(patternRemoveHtml, r'', resultContent)
resultContent = decodeHtmlSpecialCharacter(resultContent)
return resultContent
</code></pre>

<pre><code class="">
目前仅仅是删除所有的html标签，并在函数decodeHtmlSpecialCharacter中转换转义字符。

* 生成Markdown文本文件
</code></pre>

<p>def exportToMarkdown(exportDir, postdate, categories, title, content):
    titleDate = postdate.strftime(&lsquo;%Y-%m-%d&rsquo;)
    contentDate = postdate.strftime(&lsquo;%Y-%m-%d %H:%M:%S %z&rsquo;)
    filename = titleDate + &lsquo;-&rsquo; + title
    filename = repalceInvalidCharInFilename(filename)
    filepath = exportDir + &lsquo;/&rsquo; + filename + &lsquo;.markdown&rsquo;
    log(&ldquo; >> save as &rdquo; + filename)</p>

<pre><code>newFile = open(unicode(filepath, "utf8"), 'w')
newFile.write('---' + '\n')
newFile.write('layout: post' + '\n')
newFile.write('title: \"' + title + '\"\n')
newFile.write('date: ' + contentDate + '\n')
newFile.write('comments: true' + '\n')
newFile.write('categories: [' + categories + ']' + '\n')
newFile.write('tags: [' + categories + ']' + '\n')
newFile.write('description: \"' + title + '\"\n')
newFile.write('keywords: ' + categories + '\n') 
newFile.write('---' + '\n\n')
newFile.write(content)
newFile.write('\n')
newFile.close()
</code></pre>

<p>```</p>

<p>生成Markdown文本文件就很简单了，在这里我需要生成github page用的Markdown博文形式，所以内容如此，你可以根据你的需要修改为其他形式的文本内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Subline Text 2 安装 Markdown 插件]]></title>
    <link href="http://luozhaohui.github.io/blog/2014/10/05/subline-text2-markdown/"/>
    <updated>2014-10-05T22:35:02+08:00</updated>
    <id>http://luozhaohui.github.io/blog/2014/10/05/subline-text2-markdown</id>
    <content type="html"><![CDATA[<h2>简介</h2>

<p>本文介绍如何在 Subline Text 2 中安装 Markdown 插件。</p>

<!--more-->


<h2>步骤</h2>

<ol>
<li><p>按 ctrl + ` 调出 console；</p></li>
<li><p>粘贴以下代码到底部命令行并回车：</p></li>
</ol>


<pre><code class="python">    import urllib2,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();os.makedirs(ipp) if not os.path.exists(ipp) else None;open(os.path.join(ipp,pf),'wb').write(urllib2.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read())
</code></pre>

<ol>
<li><p>稍作等待，若在 output 窗口输出如下信息，表示安装成功：</p>

<blockquote><p>Please restart Sublime Text to finish installation</p></blockquote></li>
<li><p>然后重启 Subline Text 2，这时在菜单 Perferences -> package settings 中会出现 package control 项；</p></li>
<li><p>点击 package control 项(或 Mac 系统下按 cmd + shift + p，Windows 系统下按 ctrl + shift + p)，接着输入 install package，回车，稍作等待（需要连接网络）；</p></li>
<li><p>在上方的输入窗口中输入 markdown，就会列出一堆与 Markdown 相关的插件；</p></li>
<li><p>依次安装 Markdown Build 和 Markdown Preview 两个插件即可；</p></li>
<li><p>新建后缀为 .markdown 的文件，按照 <a href="http://wowubuntu.com/markdown/#blockquote">Markdown 语法</a> 规则添加内容；</p></li>
<li><p>选择菜单 Tools -> Build System -> Markdown，然后选择 Tools -> Build 编译，就会在与 .markdown 文件同目录下生成相应的 .html 文件；</p></li>
<li><p>Mac 系统下按 cmd + shift + p (Windows 系统下按 ctrl + shift + p)，输入 markdown，在自助提示列表中选择 Markdown Preview: Preview in browser，则会在浏览器中预览其效果。</p></li>
</ol>


<hr />

<p>补充：</p>

<ol>
<li><p>Mac 系统下可以安装Markdown编辑工具 <a href="http://macdown.uranusjr.com/">Macdown</a> ；</p></li>
<li><p>或直接使用在线Markdown编辑工具：<a href="http://mahua.jser.me/">mahua</a> 。</p></li>
</ol>

]]></content>
  </entry>
  
</feed>
