<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>科技 on 飘飘白云：所读，所观，所思</title>
    <link>https://luozhaohui.github.io/tags/%E7%A7%91%E6%8A%80/</link>
    <description>Recent content in 科技 on 飘飘白云：所读，所观，所思</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 22 Feb 2024 22:14:00 +0800</lastBuildDate><atom:link href="https://luozhaohui.github.io/tags/%E7%A7%91%E6%8A%80/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>人工智能：Sora 随感</title>
      <link>https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/</link>
      <pubDate>Thu, 22 Feb 2024 22:14:00 +0800</pubDate>
      
      <guid>https://luozhaohui.github.io/post/2024/2024-02-22-thinking-about-agi/</guid>
      <description>&lt;h2 id=&#34;20250210-更新&#34;&gt;2025.02.10 更新&lt;/h2&gt;
&lt;p&gt;一年前在粗略了解 Sora 之后发表的看法，对智能与涌现以及 Scaling Law 的理解上有错误和不完善之处，在此更新最新的理解。&lt;/p&gt;
&lt;h3 id=&#34;scaling-law&#34;&gt;Scaling Law&lt;/h3&gt;
&lt;p&gt;对人工智能来说，随着科技的进步，Scaling Law 不是必须，或者说曾经认为的大规模，对现在和将来来说仅仅是小菜一碟。本质上，Scaling Law 和智能也没有多大关系，可以说它仅仅是在 AI 发展早期的一个原料制约因素。&lt;/p&gt;
&lt;h3 id=&#34;智能与涌现&#34;&gt;智能与涌现&lt;/h3&gt;
&lt;p&gt;智能只与 Transformer 相关，而且涌现是必定可能的。涌现是指&lt;strong&gt;在复杂系统中，整体系统展现出无法从个体特性直接预测的新性质&lt;/strong&gt;，这些性质源于个体之间的相互作用，而不是单独个体的功能。Transformer 的如下机制，可以导致涌现或创新想法的出现。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多头注意力机制&lt;/strong&gt;：多头注意力机制允许模型在多个层次上建立词与词之间的关系。这种多层次关联可能导致模型捕捉到一些隐含的模式或语义结构，从而产生新的理解或生成新的内容。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前馈神经网络的非线性变换&lt;/strong&gt;：前馈神经网络通过非线性激活函数对注意力机制的结果进行变换（使得结果更加发散或说天马行空）。这种非线性变换可能导致模型捕捉到一些复杂的语言模式，从而生成新的内容。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多层堆叠的深度结构&lt;/strong&gt;：Transformer 通常由多个层堆叠而成，每一层都会对输入进行进一步的处理。这种多层结构可能导致模型捕捉到更高层次的语义特征，从而产生新的理解或生成新的内容。也就是说多层累积降低了有限方案的确定性，增加了发散性。比如说每一层唯一方案的确定性为 95%，那么经六层累积之后，这个确定性就降为 74% 了（95% 的 6 次方 = 73.51%）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自回归生成机制&lt;/strong&gt;：在生成任务中，Transformer 通过自回归的方式逐步生成文本。每一步生成都依赖于之前生成的内容，这种动态过程可能导致模型“变异”走上不寻常路，并逐步“自反馈”演变，产生一些意想不到的结果。这有点像演化论中的变异。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
